# üìä Guia R√°pido - Gera√ß√£o de Data Schemas

Este guia explica como gerar automaticamente os schemas de todas as tabelas do projeto ARGOS.

---

## üéØ Objetivo

Gerar arquivos de documenta√ß√£o contendo:
- `DESCRIBE FORMATTED` - estrutura completa de cada tabela
- `SELECT * FROM ... LIMIT 10` - amostra de dados reais

---

## üìã Tabelas que ser√£o documentadas

### Tabelas Originais (4)
- `nfce.nfce` - Dados brutos de NFC-e
- `niat.argos_cnpj` - Lista de CNPJs monitorados
- `niat.tabela_niat` - Al√≠quotas esperadas (IA)
- `usr_sat_ods.vw_ods_contrib` - View de contribuintes

### Tabelas Intermedi√°rias (4)
- `niat.argos_nfce_base_extraida` - Base extra√≠da e flattenida
- `niat.argos_nfce_periodo_base` - Al√≠quotas ponderadas por per√≠odo
- `niat.argos_medias_historicas_produto` - M√©dias hist√≥ricas
- `niat.argos_mudanca_comportamento` - An√°lise de mudan√ßas

### Views (1)
- `niat.argos_vw_evolucao_nfce` ‚≠ê **MAIS IMPORTANTE** - usada em todo o Streamlit

**Total: 9 tabelas/views**

---

## üöÄ Como Usar

### Op√ß√£o 1: Notebook Jupyter (RECOMENDADO)

1. Abra o notebook:
   ```bash
   jupyter notebook Gerar_Data_Schemas.ipynb
   ```

2. Execute as c√©lulas na ordem:
   - C√©lulas 1-5: Configura√ß√£o e fun√ß√µes auxiliares
   - C√©lula 6: **EXECUTAR TUDO DE UMA VEZ** (recomendado)
   - OU C√©lulas 7-8: Executar por categoria (opcional)

3. Os schemas ser√£o salvos em `./data-schemas/`

### Op√ß√£o 2: Script Python

```python
# Em um notebook PySpark ou PySpark shell:
from gerar_data_schemas import gerar_todos_schemas

# Executar
gerar_todos_schemas(spark)
```

### Op√ß√£o 3: Comandos Manuais

Se preferir executar manualmente, use o padr√£o:

```python
# DESCRIBE FORMATTED
describe_df = spark.sql("DESCRIBE FORMATTED niat.argos_vw_evolucao_nfce")
describe_df.show(1000, truncate=False)

# SELECT LIMIT 10
sample_df = spark.sql("SELECT * FROM niat.argos_vw_evolucao_nfce LIMIT 10")
sample_df.show(10, truncate=False)
```

---

## üìÅ Estrutura de Sa√≠da

Ap√≥s a execu√ß√£o, ser√° criado o diret√≥rio `data-schemas/` com:

```
data-schemas/
‚îú‚îÄ‚îÄ README.md                                    # √çndice de todos os schemas
‚îÇ
‚îú‚îÄ‚îÄ nfce_schema.txt                             # Schema: nfce.nfce
‚îú‚îÄ‚îÄ argos_cnpj_schema.txt                       # Schema: niat.argos_cnpj
‚îú‚îÄ‚îÄ tabela_niat_schema.txt                      # Schema: niat.tabela_niat
‚îú‚îÄ‚îÄ vw_ods_contrib_schema.txt                   # Schema: usr_sat_ods.vw_ods_contrib
‚îÇ
‚îú‚îÄ‚îÄ argos_nfce_base_extraida_schema.txt         # Schema: niat.argos_nfce_base_extraida
‚îú‚îÄ‚îÄ argos_nfce_periodo_base_schema.txt          # Schema: niat.argos_nfce_periodo_base
‚îú‚îÄ‚îÄ argos_medias_historicas_produto_schema.txt  # Schema: niat.argos_medias_historicas_produto
‚îú‚îÄ‚îÄ argos_mudanca_comportamento_schema.txt      # Schema: niat.argos_mudanca_comportamento
‚îÇ
‚îî‚îÄ‚îÄ argos_vw_evolucao_nfce_schema.txt          # Schema: niat.argos_vw_evolucao_nfce ‚≠ê
```

---

## üìÑ Formato dos Arquivos Gerados

Cada arquivo `*_schema.txt` cont√©m:

```
================================================================================
                        DATA SCHEMA - niat.argos_vw_evolucao_nfce
================================================================================

Categoria: VIEWS
Gerado em: 2025-11-17 10:30:00
Tabela: niat.argos_vw_evolucao_nfce

--------------------------------------------------------------------------------
                            DESCRIBE FORMATTED
--------------------------------------------------------------------------------

```sql
DESCRIBE FORMATTED niat.argos_vw_evolucao_nfce;
```

col_name        data_type       comment
cnpj            string          NULL
periodo         string          NULL
gtin            string          NULL
...

--------------------------------------------------------------------------------
                          SELECT * FROM ... LIMIT 10
--------------------------------------------------------------------------------

```sql
SELECT * FROM niat.argos_vw_evolucao_nfce LIMIT 10;
```

cnpj              periodo    gtin          ncm       produto         ...
00000000000001    202401     7891000000001 12345678  Produto X      ...
00000000000002    202401     7891000000002 87654321  Produto Y      ...
...

================================================================================
                              FIM DO SCHEMA
================================================================================
```

---

## ‚öôÔ∏è Personaliza√ß√£o

### Adicionar/Remover Tabelas

Edite o arquivo `gerar_data_schemas.py` ou o notebook, se√ß√£o de configura√ß√£o:

```python
TABELAS = {
    'ORIGINAIS': [
        'nfce.nfce',
        'niat.argos_cnpj',
        # Adicione ou remova aqui
    ],
    'INTERMEDIARIAS': [
        # ...
    ],
    'VIEWS': [
        # ...
    ]
}
```

### Alterar N√∫mero de Registros de Amostra

Por padr√£o, s√£o extra√≠dos 10 registros. Para alterar:

```python
# Altere LIMIT 10 para LIMIT 20 (ou outro valor)
sample_query = f"SELECT * FROM {tabela} LIMIT 20"
```

### Alterar Diret√≥rio de Sa√≠da

```python
# Altere o caminho do diret√≥rio
OUTPUT_DIR = './meu_diretorio_schemas'
```

---

## üîç Verifica√ß√£o

Ap√≥s a execu√ß√£o, verifique:

1. **Total de arquivos gerados**:
   ```bash
   ls -la data-schemas/ | wc -l
   # Deve mostrar 10+ arquivos (9 schemas + README.md + poss√≠veis erros)
   ```

2. **Arquivos de erro**:
   ```bash
   ls data-schemas/ERRO_*
   # Se houver arquivos, verifique permiss√µes ou se a tabela existe
   ```

3. **Tamanho dos arquivos**:
   ```bash
   du -h data-schemas/*_schema.txt
   # Cada arquivo deve ter alguns KB
   ```

---

## ‚ùå Solu√ß√£o de Problemas

### Erro: "Table not found"

**Causa**: Tabela n√£o existe ou voc√™ n√£o tem permiss√£o de acesso.

**Solu√ß√£o**:
- Verifique se a tabela existe: `spark.sql("SHOW TABLES IN niat").show()`
- Verifique permiss√µes com seu administrador de banco de dados

### Erro: "SparkSession not found"

**Causa**: O script est√° sendo executado fora de um ambiente PySpark.

**Solu√ß√£o**:
- Use o notebook Jupyter conectado ao cluster PySpark
- OU execute via `pyspark` shell
- OU inicie uma SparkSession manualmente

### Arquivos muito pequenos ou vazios

**Causa**: Tabela existe mas est√° vazia.

**Solu√ß√£o**:
- Verifique se a tabela tem dados: `spark.sql("SELECT COUNT(*) FROM tabela").show()`
- Se estiver vazia, isso √© esperado e o arquivo refletir√° essa condi√ß√£o

---

## üìä Exemplo de Uso Completo

```python
# 1. Importar bibliotecas
from pyspark.sql import SparkSession
from gerar_data_schemas import gerar_todos_schemas

# 2. Verificar SparkSession (j√° deve estar ativa em notebooks)
print(f"Spark version: {spark.version}")

# 3. Executar gera√ß√£o
gerar_todos_schemas(spark)

# 4. Verificar arquivos gerados
import os
arquivos = os.listdir('./data-schemas')
print(f"Total de arquivos gerados: {len(arquivos)}")
for arquivo in sorted(arquivos):
    print(f"  - {arquivo}")
```

---

## üìù Checklist de Execu√ß√£o

- [ ] Ambiente PySpark ativo
- [ ] Acesso ao banco de dados Impala configurado
- [ ] Permiss√µes de leitura nas tabelas
- [ ] Notebook ou script copiado para o ambiente
- [ ] Execu√ß√£o conclu√≠da sem erros
- [ ] Arquivos gerados em `./data-schemas/`
- [ ] README.md criado com √≠ndice
- [ ] Todos os schemas revisados

---

## üéØ Pr√≥ximos Passos

Ap√≥s gerar os schemas:

1. **Revisar os arquivos** em `./data-schemas/`
2. **Verificar estruturas** - conferir se os campos est√£o corretos
3. **Documentar descobertas** - adicionar coment√°rios sobre campos importantes
4. **Compartilhar** - distribuir para equipe de desenvolvimento
5. **Versionamento** - fazer commit dos schemas no reposit√≥rio Git

---

## üìö Refer√™ncias

- **Script Python**: `gerar_data_schemas.py`
- **Notebook Jupyter**: `Gerar_Data_Schemas.ipynb`
- **Documenta√ß√£o PySpark SQL**: https://spark.apache.org/docs/latest/sql-getting-started.html
- **README do projeto**: `README.md`

---

**√öltima atualiza√ß√£o**: 2025-11-17
**Autor**: Sistema ARGOS
**Vers√£o**: 1.0
